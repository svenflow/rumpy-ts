<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="color-scheme" content="dark">
    <meta name="theme-color" content="#0a0a0f">
    <title>Rayon vs pthreadpool: Threading Models for WASM</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true, theme:'dark'});</script>
    <style>
        /* CSS Variables - Dark Theme with Purple Accent */
        :root {
            --bg-primary: #0a0a0f;
            --bg-secondary: #12121a;
            --bg-card: #1a1a24;
            --bg-code: #0d0d14;
            --text-primary: #f0f0f5;
            --text-secondary: #9ca3af;
            --text-muted: #6b7280;
            --accent: #8b5cf6;
            --accent-light: #a78bfa;
            --accent-dim: rgba(139, 92, 246, 0.15);
            --success: #10b981;
            --warning: #f59e0b;
            --danger: #ef4444;
            --border: rgba(255, 255, 255, 0.08);
            --border-accent: rgba(139, 92, 246, 0.3);
            --shadow: 0 4px 24px rgba(0, 0, 0, 0.4);
            --shadow-lg: 0 8px 48px rgba(0, 0, 0, 0.5);
            --radius: 12px;
            --radius-sm: 8px;
        }

        /* Reset & Base */
        *, *::before, *::after {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: var(--bg-primary);
            color: var(--text-primary);
            line-height: 1.7;
            font-size: 16px;
            min-height: 100vh;
        }

        /* Layout */
        .container {
            max-width: 1100px;
            margin: 0 auto;
            padding: 2rem;
        }

        /* Hero Section */
        .hero {
            text-align: center;
            padding: 4rem 2rem 3rem;
            background: linear-gradient(180deg, var(--bg-secondary) 0%, var(--bg-primary) 100%);
            border-bottom: 1px solid var(--border);
            margin-bottom: 3rem;
        }

        .hero h1 {
            font-size: clamp(2rem, 5vw, 3.5rem);
            font-weight: 800;
            background: linear-gradient(135deg, var(--text-primary) 0%, var(--accent-light) 50%, var(--accent) 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            margin-bottom: 1rem;
            letter-spacing: -0.02em;
        }

        .hero .subtitle {
            font-size: 1.25rem;
            color: var(--text-secondary);
            max-width: 700px;
            margin: 0 auto;
        }

        .hero .badge {
            display: inline-block;
            background: var(--accent-dim);
            color: var(--accent-light);
            padding: 0.5rem 1rem;
            border-radius: 2rem;
            font-size: 0.875rem;
            font-weight: 500;
            margin-top: 1.5rem;
            border: 1px solid var(--border-accent);
        }

        /* Section Cards */
        .section {
            background: var(--bg-card);
            border: 1px solid var(--border);
            border-radius: var(--radius);
            padding: 2rem;
            margin-bottom: 2rem;
            box-shadow: var(--shadow);
        }

        .section h2 {
            font-size: 1.75rem;
            font-weight: 700;
            color: var(--text-primary);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            gap: 0.75rem;
        }

        .section h2 .icon {
            width: 32px;
            height: 32px;
            background: var(--accent-dim);
            border-radius: var(--radius-sm);
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1rem;
        }

        .section h3 {
            font-size: 1.25rem;
            font-weight: 600;
            color: var(--text-primary);
            margin: 1.5rem 0 0.75rem;
        }

        .section p {
            color: var(--text-secondary);
            margin-bottom: 1rem;
        }

        /* Comparison Grid */
        .comparison-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 1.5rem;
            margin-top: 1.5rem;
        }

        .comparison-card {
            background: var(--bg-secondary);
            border: 1px solid var(--border);
            border-radius: var(--radius);
            padding: 1.5rem;
            position: relative;
            overflow: hidden;
        }

        .comparison-card::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            height: 3px;
        }

        .comparison-card.rayon::before {
            background: linear-gradient(90deg, #f59e0b, #f97316);
        }

        .comparison-card.pthreadpool::before {
            background: linear-gradient(90deg, #10b981, #06b6d4);
        }

        .comparison-card h4 {
            font-size: 1.25rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
        }

        .comparison-card.rayon h4 {
            color: #f59e0b;
        }

        .comparison-card.pthreadpool h4 {
            color: #10b981;
        }

        .comparison-card ul {
            list-style: none;
            margin-top: 1rem;
        }

        .comparison-card li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
            color: var(--text-secondary);
            border-bottom: 1px solid var(--border);
        }

        .comparison-card li:last-child {
            border-bottom: none;
        }

        .comparison-card li::before {
            content: '';
            position: absolute;
            left: 0;
            top: 50%;
            transform: translateY(-50%);
            width: 6px;
            height: 6px;
            border-radius: 50%;
        }

        .comparison-card.rayon li::before {
            background: #f59e0b;
        }

        .comparison-card.pthreadpool li::before {
            background: #10b981;
        }

        /* Mermaid Diagrams */
        .mermaid-container {
            background: var(--bg-secondary);
            border: 1px solid var(--border);
            border-radius: var(--radius);
            padding: 1.5rem;
            margin: 1.5rem 0;
            overflow-x: auto;
        }

        .mermaid {
            display: flex;
            justify-content: center;
        }

        /* Tables */
        .table-container {
            overflow-x: auto;
            margin: 1.5rem 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            font-size: 0.9rem;
        }

        th, td {
            padding: 0.875rem 1rem;
            text-align: left;
            border-bottom: 1px solid var(--border);
        }

        th {
            background: var(--bg-secondary);
            color: var(--text-primary);
            font-weight: 600;
            text-transform: uppercase;
            font-size: 0.75rem;
            letter-spacing: 0.05em;
        }

        td {
            color: var(--text-secondary);
        }

        tr:hover td {
            background: rgba(139, 92, 246, 0.05);
        }

        .faster {
            color: var(--success);
            font-weight: 600;
        }

        .slower {
            color: var(--danger);
            font-weight: 500;
        }

        .neutral {
            color: var(--warning);
        }

        /* Code Blocks */
        pre {
            background: var(--bg-code);
            border: 1px solid var(--border);
            border-radius: var(--radius-sm);
            padding: 1.25rem;
            overflow-x: auto;
            font-size: 0.875rem;
            line-height: 1.6;
            margin: 1rem 0;
        }

        code {
            font-family: 'JetBrains Mono', 'Fira Code', 'Monaco', monospace;
        }

        :not(pre) > code {
            background: var(--accent-dim);
            color: var(--accent-light);
            padding: 0.2em 0.4em;
            border-radius: 4px;
            font-size: 0.9em;
        }

        /* Keyword highlighting */
        .kw { color: #c792ea; }
        .fn { color: #82aaff; }
        .st { color: #c3e88d; }
        .cm { color: #546e7a; }
        .nm { color: #f78c6c; }

        /* Callout Boxes */
        .callout {
            padding: 1.25rem;
            border-radius: var(--radius-sm);
            margin: 1.5rem 0;
            border-left: 4px solid;
        }

        .callout.info {
            background: rgba(59, 130, 246, 0.1);
            border-color: #3b82f6;
        }

        .callout.success {
            background: rgba(16, 185, 129, 0.1);
            border-color: var(--success);
        }

        .callout.warning {
            background: rgba(245, 158, 11, 0.1);
            border-color: var(--warning);
        }

        .callout.danger {
            background: rgba(239, 68, 68, 0.1);
            border-color: var(--danger);
        }

        .callout-title {
            font-weight: 600;
            margin-bottom: 0.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .callout.info .callout-title { color: #3b82f6; }
        .callout.success .callout-title { color: var(--success); }
        .callout.warning .callout-title { color: var(--warning); }
        .callout.danger .callout-title { color: var(--danger); }

        /* Key Insight Highlight */
        .insight {
            background: linear-gradient(135deg, var(--accent-dim), rgba(139, 92, 246, 0.05));
            border: 1px solid var(--border-accent);
            border-radius: var(--radius);
            padding: 1.5rem;
            margin: 1.5rem 0;
        }

        .insight-title {
            color: var(--accent-light);
            font-weight: 700;
            font-size: 1.1rem;
            margin-bottom: 0.5rem;
        }

        /* Stats Grid */
        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 1rem;
            margin: 1.5rem 0;
        }

        .stat {
            background: var(--bg-secondary);
            border: 1px solid var(--border);
            border-radius: var(--radius-sm);
            padding: 1.25rem;
            text-align: center;
        }

        .stat-value {
            font-size: 2rem;
            font-weight: 800;
            color: var(--accent-light);
            line-height: 1.2;
        }

        .stat-label {
            font-size: 0.8rem;
            color: var(--text-muted);
            text-transform: uppercase;
            letter-spacing: 0.05em;
            margin-top: 0.25rem;
        }

        /* Navigation */
        .nav {
            position: fixed;
            top: 1rem;
            right: 1rem;
            display: flex;
            flex-direction: column;
            gap: 0.5rem;
            z-index: 100;
        }

        .nav a {
            width: 40px;
            height: 40px;
            background: var(--bg-card);
            border: 1px solid var(--border);
            border-radius: var(--radius-sm);
            display: flex;
            align-items: center;
            justify-content: center;
            color: var(--text-secondary);
            text-decoration: none;
            transition: all 0.2s ease;
            font-size: 1.25rem;
        }

        .nav a:hover {
            background: var(--accent-dim);
            color: var(--accent-light);
            border-color: var(--border-accent);
        }

        /* Footer */
        .footer {
            text-align: center;
            padding: 2rem;
            color: var(--text-muted);
            font-size: 0.875rem;
            border-top: 1px solid var(--border);
            margin-top: 3rem;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }

            .section {
                padding: 1.25rem;
            }

            .nav {
                display: none;
            }

            .comparison-grid {
                grid-template-columns: 1fr;
            }
        }

        /* Animations */
        @media (prefers-reduced-motion: no-preference) {
            .section {
                animation: fadeIn 0.5s ease;
            }

            @keyframes fadeIn {
                from {
                    opacity: 0;
                    transform: translateY(10px);
                }
                to {
                    opacity: 1;
                    transform: translateY(0);
                }
            }
        }
    </style>
</head>
<body>

<nav class="nav" aria-label="Quick navigation">
    <a href="#top" title="Back to top">^</a>
</nav>

<header class="hero" id="top">
    <h1>Rayon vs pthreadpool</h1>
    <p class="subtitle">Understanding threading models for WebAssembly and why synchronization strategy matters for GEMM performance</p>
    <span class="badge">Deep Dive: WASM Threading</span>
</header>

<main class="container">

    <!-- What is a Thread Pool -->
    <section class="section" id="intro">
        <h2><span class="icon">1</span> What is a Thread Pool?</h2>
        <p>
            A <strong>thread pool</strong> is a collection of pre-spawned worker threads that wait for tasks. Instead of creating and destroying threads for each parallel operation, a pool keeps threads alive and reuses them. This dramatically reduces overhead for repeated parallel work.
        </p>

        <div class="mermaid-container">
            <pre class="mermaid">
flowchart LR
    subgraph Pool["Thread Pool"]
        T1["Thread 1"]
        T2["Thread 2"]
        T3["Thread 3"]
        T4["Thread 4"]
    end

    Q["Task Queue"] --> Pool
    Pool --> R["Results"]

    style Pool fill:#1a1a24,stroke:#8b5cf6,stroke-width:2px
    style Q fill:#12121a,stroke:#f59e0b,stroke-width:2px
    style R fill:#12121a,stroke:#10b981,stroke-width:2px
            </pre>
        </div>

        <div class="callout info">
            <div class="callout-title">Why Thread Pools Matter for GEMM</div>
            <p>Matrix multiplication (GEMM) is the core operation in neural networks. A single forward pass might call GEMM thousands of times. If each call spends 50-200 microseconds on thread overhead, that adds up to significant latency.</p>
        </div>

        <div class="stats-grid">
            <div class="stat">
                <div class="stat-value">100ms</div>
                <div class="stat-label">Thread spawn cost</div>
            </div>
            <div class="stat">
                <div class="stat-value">0.1ms</div>
                <div class="stat-label">Pool wake cost</div>
            </div>
            <div class="stat">
                <div class="stat-value">1000x</div>
                <div class="stat-label">Overhead reduction</div>
            </div>
        </div>
    </section>

    <!-- Rayon -->
    <section class="section" id="rayon">
        <h2><span class="icon">2</span> How Rayon Works</h2>
        <p>
            <strong>Rayon</strong> is Rust's most popular parallelism library. It uses a <em>work-stealing</em> scheduler with a <em>fork-join</em> model, inspired by Cilk and Java's ForkJoinPool.
        </p>

        <h3>Work-Stealing Model</h3>
        <p>Each thread has its own deque (double-ended queue) of tasks. Threads push/pop from their own deque, but when idle, they <em>steal</em> from other threads' deques.</p>

        <div class="mermaid-container">
            <pre class="mermaid">
flowchart TB
    subgraph Thread1["Thread 1"]
        D1["Deque: [A, B, C]"]
    end

    subgraph Thread2["Thread 2"]
        D2["Deque: [D]"]
    end

    subgraph Thread3["Thread 3 (idle)"]
        D3["Deque: []"]
    end

    D1 -.->|"steal C"| D3

    style Thread1 fill:#1a1a24,stroke:#f59e0b,stroke-width:2px
    style Thread2 fill:#1a1a24,stroke:#f59e0b,stroke-width:2px
    style Thread3 fill:#1a1a24,stroke:#6b7280,stroke-width:2px,stroke-dasharray: 5 5
            </pre>
        </div>

        <h3>Fork-Join Pattern</h3>
        <p>Work is recursively split until pieces are small enough. This creates a tree of tasks that naturally load-balances across threads.</p>

        <div class="mermaid-container">
            <pre class="mermaid">
graph TD
    A["parallel_for(0..1024)"] --> B["fork: 0..512"]
    A --> C["fork: 512..1024"]
    B --> D["0..256"]
    B --> E["256..512"]
    C --> F["512..768"]
    C --> G["768..1024"]
    D --> H["join"]
    E --> H
    F --> I["join"]
    G --> I
    H --> J["final join"]
    I --> J

    style A fill:#f59e0b,stroke:#f59e0b,color:#000
    style J fill:#10b981,stroke:#10b981,color:#000
            </pre>
        </div>

        <h3>Rayon's Synchronization</h3>
        <pre><code><span class="cm">// Simplified Rayon latch mechanism</span>
<span class="kw">struct</span> <span class="fn">Latch</span> {
    counter: AtomicUsize,
    parker: Mutex&lt;()&gt;,
    condvar: Condvar,
}

<span class="kw">impl</span> <span class="fn">Latch</span> {
    <span class="kw">fn</span> <span class="fn">wait</span>(&self) {
        <span class="cm">// Spin briefly, then park on condvar</span>
        <span class="kw">while</span> self.counter.load(Relaxed) != <span class="nm">0</span> {
            spin_loop();  <span class="cm">// Burns CPU cycles</span>
        }
        <span class="cm">// Fall back to condvar wait</span>
        <span class="kw">let</span> guard = self.parker.lock();
        self.condvar.wait(guard);
    }
}</code></pre>

        <div class="callout warning">
            <div class="callout-title">WASM Problem: Expensive Atomics</div>
            <p>WebAssembly atomics go through JavaScript's <code>Atomics.wait()</code> and <code>Atomics.notify()</code>. On Apple Silicon, this has ~10x higher overhead than native futex calls, making work-stealing latches expensive.</p>
        </div>
    </section>

    <!-- pthreadpool -->
    <section class="section" id="pthreadpool">
        <h2><span class="icon">3</span> How pthreadpool Works</h2>
        <p>
            <strong>pthreadpool</strong> is a C library designed specifically for ML workloads. It's used by XNNPACK (TensorFlow.js WASM backend), PyTorch, and ONNX Runtime. Its design prioritizes low latency over flexibility.
        </p>

        <h3>Persistent Threads with Futex Wake</h3>
        <p>Unlike Rayon's work-stealing, pthreadpool keeps threads permanently parked, waking them only when work arrives. The key insight: <em>waking a parked thread is cheaper than coordinating work-stealing</em>.</p>

        <div class="mermaid-container">
            <pre class="mermaid">
sequenceDiagram
    participant M as Main Thread
    participant W1 as Worker 1
    participant W2 as Worker 2
    participant W3 as Worker 3

    Note over W1,W3: Threads parked on futex

    M->>M: Store command + params
    M->>W1: futex_wake()
    M->>W2: futex_wake()
    M->>W3: futex_wake()

    activate W1
    activate W2
    activate W3

    W1->>W1: Process items 0-99
    W2->>W2: Process items 100-199
    W3->>W3: Process items 200-299
    M->>M: Process items 300-399

    Note over M: Caller participates!

    W1-->>M: atomic_dec(active)
    W2-->>M: atomic_dec(active)
    W3-->>M: atomic_dec(active)

    deactivate W1
    deactivate W2
    deactivate W3

    M->>M: All done, return
            </pre>
        </div>

        <h3>Static Work Distribution + Work Stealing</h3>
        <p>Work is <em>pre-divided</em> among threads at dispatch time. Each thread owns a range. If a thread finishes early, it steals from the <em>end</em> of other threads' ranges.</p>

        <div class="mermaid-container">
            <pre class="mermaid">
flowchart LR
    subgraph Initial["Initial Distribution"]
        R1["Thread 0: [0-99]"]
        R2["Thread 1: [100-199]"]
        R3["Thread 2: [200-299]"]
    end

    subgraph Processing["After Thread 0 Finishes"]
        R1b["Thread 0: done"]
        R2b["Thread 1: [100-180]"]
        R3b["Thread 2: [200-280]"]
    end

    R1 --> R1b
    R2 --> R2b
    R3 --> R3b

    R1b -.->|"steal 299,298..."| R3b
    R1b -.->|"steal 199,198..."| R2b

    style R1b fill:#10b981,stroke:#10b981,color:#000
            </pre>
        </div>

        <h3>Wait-Free Work Item Claiming</h3>
        <p>The key optimization: work items are claimed via <code>atomic_decrement</code>, not compare-and-swap. This is wait-free - threads never block on each other when claiming work.</p>

        <pre><code><span class="cm">// Fastpath: atomic decrement with underflow detection</span>
<span class="kw">const</span> threshold: isize = -(threads_count <span class="kw">as</span> isize);

<span class="kw">while</span> atomic_fetch_sub(&range_length, <span class="nm">1</span>) > threshold {
    <span class="kw">let</span> index = atomic_fetch_add(&range_start, <span class="nm">1</span>);
    task(context, index);  <span class="cm">// Process one item</span>
}
<span class="cm">// Underflow detected = no more work in this range</span></code></pre>

        <h3>Caller Participates</h3>
        <p>Unlike Rayon where the main thread just waits, in pthreadpool the calling thread <em>also processes work</em>. For N threads, you get N+1 workers (including caller).</p>

        <div class="insight">
            <div class="insight-title">Key Insight: Why Caller Participation Matters</div>
            <p>For short tasks (< 1ms), thread wake latency dominates. If the main thread processes 1/4 of the work while workers wake up, you've hidden most of the wake latency. This is pthreadpool's secret sauce for GEMM kernels.</p>
        </div>
    </section>

    <!-- Head-to-Head Comparison -->
    <section class="section" id="comparison">
        <h2><span class="icon">4</span> Head-to-Head Comparison</h2>

        <div class="comparison-grid">
            <div class="comparison-card rayon">
                <h4>Rayon</h4>
                <p>General-purpose work-stealing</p>
                <ul>
                    <li>Fork-join model with recursive splitting</li>
                    <li>Work-stealing between thread deques</li>
                    <li>Latch-based synchronization</li>
                    <li>Main thread waits for completion</li>
                    <li>Dynamic task creation (flexible)</li>
                    <li>Higher overhead per task</li>
                </ul>
            </div>
            <div class="comparison-card pthreadpool">
                <h4>pthreadpool</h4>
                <p>Optimized for ML workloads</p>
                <ul>
                    <li>Static work distribution</li>
                    <li>Futex-based thread parking</li>
                    <li>Wait-free work item claiming</li>
                    <li>Caller participates in work</li>
                    <li>Pre-defined parallelization patterns</li>
                    <li>Minimal overhead per dispatch</li>
                </ul>
            </div>
        </div>

        <div class="mermaid-container">
            <pre class="mermaid">
graph TB
    subgraph Rayon["Rayon: Work Stealing"]
        R1["Task spawned"] --> R2["Push to deque"]
        R2 --> R3["Fork recursively"]
        R3 --> R4["Threads steal work"]
        R4 --> R5["Latches coordinate"]
        R5 --> R6["Join results"]
    end

    subgraph PTP["pthreadpool: Static Dispatch"]
        P1["parallelize_2d()"] --> P2["Divide range"]
        P2 --> P3["Wake all threads"]
        P3 --> P4["Each processes range"]
        P4 --> P5["Steal from ends"]
        P5 --> P6["atomic_dec(active)"]
    end

    style R6 fill:#f59e0b,stroke:#f59e0b,color:#000
    style P6 fill:#10b981,stroke:#10b981,color:#000
            </pre>
        </div>

        <h3>Synchronization Overhead Visualization</h3>
        <div class="mermaid-container">
            <pre class="mermaid">
gantt
    title Thread Activity Timeline (100 items, 4 threads)
    dateFormat X
    axisFormat %L

    section Rayon
    Fork overhead     :crit, r1, 0, 20
    Thread 0 work     :active, r2, 20, 60
    Thread 1 steal    :r3, 25, 55
    Thread 2 steal    :r4, 30, 65
    Thread 3 steal    :r5, 35, 60
    Join overhead     :crit, r6, 65, 80

    section pthreadpool
    Wake threads      :crit, p1, 0, 5
    Caller work       :active, p2, 5, 45
    Thread 0 work     :p3, 8, 45
    Thread 1 work     :p4, 10, 48
    Thread 2 work     :p5, 12, 50
    Complete          :done, p6, 50, 52
            </pre>
        </div>
    </section>

    <!-- Benchmark Data -->
    <section class="section" id="benchmarks">
        <h2><span class="icon">5</span> Benchmark Data: GEMM Performance</h2>
        <p>Real benchmarks from rumpy-ts comparing Rayon-based parallel matmul vs TensorFlow.js XNNPACK (which uses pthreadpool). Both using 14 threads on Apple M4.</p>

        <div class="callout danger">
            <div class="callout-title">The Performance Gap</div>
            <p>Despite using the same number of threads and similar SIMD kernels, Rayon-based parallel GEMM is <strong>1.5-5x slower</strong> than pthreadpool-based XNNPACK at most matrix sizes.</p>
        </div>

        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Matrix Size</th>
                        <th>Rayon (V2)</th>
                        <th>pthreadpool (TFJS)</th>
                        <th>Ratio</th>
                        <th>Analysis</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>32x32</td>
                        <td>0.007ms</td>
                        <td>0.020ms</td>
                        <td class="faster">2.9x faster</td>
                        <td>Small matrix - overhead dominates both</td>
                    </tr>
                    <tr>
                        <td>64x64</td>
                        <td>0.024ms</td>
                        <td>0.019ms</td>
                        <td class="slower">1.3x slower</td>
                        <td>Thread coordination overhead</td>
                    </tr>
                    <tr>
                        <td>100x100</td>
                        <td>0.166ms</td>
                        <td>0.031ms</td>
                        <td class="slower">5.4x slower</td>
                        <td>Rayon's latch overhead dominates</td>
                    </tr>
                    <tr>
                        <td>128x128</td>
                        <td>0.223ms</td>
                        <td>0.041ms</td>
                        <td class="slower">5.4x slower</td>
                        <td>Same pattern</td>
                    </tr>
                    <tr>
                        <td>256x256</td>
                        <td>0.318ms</td>
                        <td>0.116ms</td>
                        <td class="slower">2.7x slower</td>
                        <td>Gap narrows with more compute</td>
                    </tr>
                    <tr>
                        <td>512x512</td>
                        <td>1.17ms</td>
                        <td>0.74ms</td>
                        <td class="slower">1.6x slower</td>
                        <td>Compute starts to dominate</td>
                    </tr>
                    <tr>
                        <td>768x768</td>
                        <td>3.41ms</td>
                        <td>2.31ms</td>
                        <td class="slower">1.5x slower</td>
                        <td>Consistent 50% overhead</td>
                    </tr>
                    <tr>
                        <td>1024x1024</td>
                        <td>8.95ms</td>
                        <td>5.35ms</td>
                        <td class="slower">1.7x slower</td>
                        <td>Gap persists at scale</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <h3>Why the Gap at Medium Sizes?</h3>
        <p>The 100-256 range shows the worst gap (5x slower). At these sizes:</p>
        <ul style="color: var(--text-secondary); padding-left: 1.5rem; margin: 1rem 0;">
            <li>Work per thread is small (~25-65 items per thread)</li>
            <li>Rayon's fork-join creates many latches</li>
            <li>pthreadpool's caller participation hides wake latency</li>
            <li>Work-stealing overhead exceeds work saved</li>
        </ul>

        <div class="insight">
            <div class="insight-title">The 5.4x Gap Explained</div>
            <p>At 128x128, total compute time is ~40us. Rayon spends ~180us on thread coordination (fork, latch, join). pthreadpool spends ~5us (futex wake) + caller does work while threads wake. The coordination overhead IS the performance gap.</p>
        </div>
    </section>

    <!-- WASM Considerations -->
    <section class="section" id="wasm">
        <h2><span class="icon">6</span> WASM-Specific Considerations</h2>
        <p>WebAssembly adds unique constraints that make threading model choice even more critical.</p>

        <h3>WASM Threading Architecture</h3>
        <div class="mermaid-container">
            <pre class="mermaid">
flowchart TB
    subgraph Browser["Browser"]
        subgraph Main["Main Thread"]
            JS["JavaScript"] --> WASM1["WASM Module"]
        end

        subgraph Workers["Web Workers"]
            W1["Worker 1"] --> WASM2["WASM Instance"]
            W2["Worker 2"] --> WASM3["WASM Instance"]
        end

        SAB["SharedArrayBuffer"]
    end

    WASM1 <-.-> SAB
    WASM2 <-.-> SAB
    WASM3 <-.-> SAB

    style SAB fill:#8b5cf6,stroke:#8b5cf6,color:#fff
    style Main fill:#1a1a24,stroke:#f59e0b
    style Workers fill:#1a1a24,stroke:#10b981
            </pre>
        </div>

        <h3>WASM Atomics Overhead</h3>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Operation</th>
                        <th>Native (ns)</th>
                        <th>WASM x86 (ns)</th>
                        <th>WASM Apple Silicon (ns)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>atomic_load</td>
                        <td>~1</td>
                        <td>~5</td>
                        <td>~10</td>
                    </tr>
                    <tr>
                        <td>atomic_cas</td>
                        <td>~10</td>
                        <td>~50</td>
                        <td>~150</td>
                    </tr>
                    <tr>
                        <td>Atomics.wait()</td>
                        <td>~1,000</td>
                        <td>~5,000</td>
                        <td class="slower">~50,000</td>
                    </tr>
                    <tr>
                        <td>Atomics.notify()</td>
                        <td>~500</td>
                        <td>~2,000</td>
                        <td class="slower">~20,000</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="callout warning">
            <div class="callout-title">Apple Silicon + WASM = Slow Atomics</div>
            <p>Apple Silicon has notoriously slow WASM atomics - 10-50x slower than x86. This is a known issue with V8/JSC on ARM. The wasm-bindgen-rayon library has <a href="https://github.com/GoogleChromeLabs/wasm-bindgen-rayon/issues/16" style="color: var(--accent-light);">GitHub issues</a> tracking this.</p>
        </div>

        <h3>Why pthreadpool is Better for WASM</h3>
        <div class="comparison-grid">
            <div class="comparison-card pthreadpool">
                <h4>Fewer Atomic Operations</h4>
                <ul>
                    <li>One futex wake per dispatch</li>
                    <li>Wait-free work claiming (no CAS loops)</li>
                    <li>Single atomic_dec for completion</li>
                    <li>No latch allocation/deallocation</li>
                </ul>
            </div>
            <div class="comparison-card pthreadpool">
                <h4>Better for Short Tasks</h4>
                <ul>
                    <li>Caller participation hides wake latency</li>
                    <li>No recursive fork overhead</li>
                    <li>Pre-computed work ranges</li>
                    <li>Designed for ML workloads (GEMM)</li>
                </ul>
            </div>
        </div>

        <h3>pthreadpool's WASM Backend</h3>
        <pre><code><span class="cm">// pthreadpool uses emscripten_futex for WASM</span>
<span class="kw">fn</span> <span class="fn">futex_wait</span>(addr: &AtomicU32, expected: u32) {
    <span class="cm">// Maps to Atomics.wait() but with spin-wait first</span>
    <span class="kw">for</span> _ <span class="kw">in</span> <span class="nm">0</span>..<span class="nm">1000</span> {
        <span class="kw">if</span> addr.load(Relaxed) != expected {
            <span class="kw">return</span>;  <span class="cm">// Avoid expensive wait syscall</span>
        }
        spin_loop();
    }
    emscripten_futex_wait(addr, expected, -<span class="nm">1</span>);
}

<span class="kw">fn</span> <span class="fn">futex_wake</span>(addr: &AtomicU32) {
    emscripten_futex_wake(addr, <span class="nm">1</span>);  <span class="cm">// Wake one thread</span>
}</code></pre>
    </section>

    <!-- Recommendations -->
    <section class="section" id="recommendations">
        <h2><span class="icon">7</span> Recommendations</h2>

        <div class="callout success">
            <div class="callout-title">When to Use pthreadpool</div>
            <ul style="color: var(--text-secondary); padding-left: 1rem; margin: 0.5rem 0;">
                <li>GEMM and convolution kernels</li>
                <li>Short, repeated parallel operations</li>
                <li>WebAssembly builds (especially Apple Silicon)</li>
                <li>When latency matters more than flexibility</li>
                <li>ML inference with fixed operation patterns</li>
            </ul>
        </div>

        <div class="callout info">
            <div class="callout-title">When Rayon is Still Good</div>
            <ul style="color: var(--text-secondary); padding-left: 1rem; margin: 0.5rem 0;">
                <li>Native Rust code (not WASM)</li>
                <li>Dynamic/irregular parallelism</li>
                <li>Long-running tasks (> 10ms)</li>
                <li>When ergonomics matter more than raw speed</li>
                <li>Recursive divide-and-conquer algorithms</li>
            </ul>
        </div>

        <h3>For rumpy-ts Specifically</h3>
        <p>Based on our benchmarks, the path forward is clear:</p>
        <ol style="color: var(--text-secondary); padding-left: 1.5rem; margin: 1rem 0;">
            <li><strong>Implement pthreadpool-rs</strong> - A pure Rust port of pthreadpool</li>
            <li><strong>Use for GEMM kernels</strong> - Replace Rayon in <code>matmulF32Parallel</code></li>
            <li><strong>Keep Rayon for other ops</strong> - Element-wise, reductions where overhead is hidden</li>
            <li><strong>Hybrid approach</strong> - pthreadpool for hot paths, Rayon for convenience</li>
        </ol>

        <div class="insight">
            <div class="insight-title">Expected Improvement</div>
            <p>Switching from Rayon to pthreadpool for GEMM should bring our parallel matmul within 10-20% of TensorFlow.js XNNPACK performance, matching the single-threaded parity we already achieved.</p>
        </div>
    </section>

    <!-- Summary -->
    <section class="section" id="summary">
        <h2><span class="icon">8</span> Summary</h2>

        <div class="mermaid-container">
            <pre class="mermaid">
mindmap
  root((Threading<br>for WASM))
    Rayon
      Work Stealing
      Fork-Join
      Flexible
      Higher Overhead
      Best for Native
    pthreadpool
      Static Distribution
      Futex Wake
      Caller Participates
      Wait-Free Claiming
      Best for WASM GEMM
    Key Insight
      Thread coordination cost
      dominates short tasks
      in WASM
            </pre>
        </div>

        <div class="stats-grid">
            <div class="stat">
                <div class="stat-value">5x</div>
                <div class="stat-label">Gap at 128x128</div>
            </div>
            <div class="stat">
                <div class="stat-value">1.5x</div>
                <div class="stat-label">Gap at 1024x1024</div>
            </div>
            <div class="stat">
                <div class="stat-value">50us</div>
                <div class="stat-label">Atomics.wait() on M4</div>
            </div>
            <div class="stat">
                <div class="stat-value">5us</div>
                <div class="stat-label">pthreadpool wake</div>
            </div>
        </div>

        <p style="text-align: center; font-size: 1.1rem; margin-top: 2rem;">
            For WASM GEMM workloads, <strong style="color: var(--success);">pthreadpool</strong> is the clear winner.<br>
            <span style="color: var(--text-muted);">The synchronization model matters more than the threading library.</span>
        </p>
    </section>

</main>

<footer class="footer">
    <p>Generated for rumpy-ts - A Rust/WASM numeric computing library</p>
    <p style="margin-top: 0.5rem;">Research based on GEMM optimization work, February 2026</p>
</footer>

</body>
</html>
